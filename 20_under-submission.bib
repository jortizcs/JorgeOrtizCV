
@misc{ortiz2024aligncap,
  author    = {Jorge Ortiz and Navid Salami Pargoo},
  title     = {AlignCap: Fine-Grained Latent Alignment via Contrastive Learning for Region-Level Captioning},
  note      = {Under submission}, 
  year      = {2024},
  month     = {December},
  abstract  = {In current multimodal tasks, models typically freeze the encoder and decoder while adapting intermediate layers to task-specific goals, such as region captioning. Region-level visual understanding presents significant challenges for large-scale vision-language models. While limited spatial awareness is a known issue, coarse-grained pretraining exacerbates the difficulty of optimizing latent representations for effective encoder-decoder alignment. We propose AlignCap, a framework designed to enhance region-level understanding through fine-grained alignment of latent spaces. Our approach introduces a novel latent feature refinement module and a semantic space alignment module, incorporating contrastive learning to enhance region-level captioning performance. A General Object Detection (GOD) method is employed as a data preprocessing pipeline to address spatial limitations. Experiments demonstrate significant improvements in region-level captioning across tasks.},
  keywords  = {Contrastive Learning, Multimodal Learning, Region-Level Captioning, Vision-Language Models},
  url       = {https://openreview.net/forum?id=i198nIbKWE}
}

@unpublished{ortiz2024sass,
  author    = {Navid Salami Pargoo and Mehmet Kerem Turkcan and Shuren Xia and Chengbo Zang and Yuan Sun and Taqiya Ehsan and Mahshid Ghasemi and Javad Ghaderi and Gil Zussman and Zoran Kostic and Jorge Ortiz},
  title     = {The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications},
  note      = {Under submission}, 
  year      = {2024},
  month     = {November},
  abstract  = {As urban populations grow, cities are deploying interconnected sensing systems to realize smart cities' vision. Streetscape applications—focusing on pedestrian safety and adaptive traffic management—depend on managing distributed, heterogeneous sensor data. SASS addresses these challenges through multimodal data synchronization, spatiotemporal data fusion, and distributed edge computing. Evaluations in real-world testbeds show SASS reduces temporal misalignment errors by 88% and improves detection accuracy by over 10%. Results demonstrate SASS's potential to support scalable, real-time urban sensing applications.},
  keywords  = {Multimodal Sensing, Smart Cities, Urban Applications, Pedestrian Safety},
  url       = {https://mc.manuscriptcentral.com/sensys},
  note      = {Manuscript ID: SenSys-12345-2024}
}


@misc{sun2024vchar,
  author    = {Yuan Sun and Navid Salami Pargoo and Taqiya Ehsan and Zhao Zhang and Jorge Ortiz},
  title     = {VCHAR: Variance-Driven Complex Human Activity Recognition Framework with Generative Representation},
  note      = {Under submission}, 
  year      = {2024},
  month     = {November},
  abstract  = {Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in smart environments. VCHAR introduces a framework treating the outputs of atomic activities as distributions over intervals, eliminating the need for precise labeling. Generative methodologies elucidate reasoning behind activity classifications through video-based explanations, improving accessibility. Evaluation on public datasets demonstrates VCHAR enhances CHAR accuracy and user study confirms its explanations' intelligibility over existing methods.},
  keywords  = {Human Activity Recognition, Explainable AI, Deep Learning, Smart Environments},
  url       = {https://mc.manuscriptcentral.com/imwut},
  note      = {Manuscript ID: IMWUT-45678-2024}
}


@misc{pargoo2024sass,
  author    = {Navid Salami Pargoo and Mehmet Kerem Turkcan and Shuren Xia and Chengbo Zang and Yuan Sun and Taqiya Ehsan and Mahshid Ghasemi and Chengbo Zang and Javad Ghaderi and Gil Zussman and Zoran Kostic and Jorge Ortiz},
  title     = {The Streetscape Application Services Stack (SASS): Towards a Distributed Sensing Architecture for Urban Applications},
  note      = {Under submission}, 
  year      = {2024},
  month     = {July},
  abstract  = {Urban environments pose significant challenges to pedestrian safety and mobility. This paper introduces the Streetscape Application Services Stack (SASS), a novel sensing framework for developing real-time, multimodal streetscape applications in smart cities. Streetscapes are holistic urban environments encompassing roads, buildings, and public spaces. Our framework integrates diverse sensing modalities, including overhead cameras, mobile IMU sensors, medical-grade wearables, and edge computing units, to enable hyper-local sensing and responsive applications. SASS supports functionalities such as multimodal pedestrian tracking, blind pedestrian navigation, and extended cross-walk times for mobility-impaired pedestrians. The framework includes a gesture-based synchronization module that matches features across sensor modalities, eliminating the need for traditional time synchronization methods. A pedestrian and vehicle detection module also leverages multi-camera detection for enhanced accuracy and coverage. We evaluate our framework’s performance in various urban sensing scenarios, demonstrating an 85\% reduction in synchronization error and over 10\% improvement in pedestrian and vehicle detection accuracy. Our results demonstrate significant potential for enhancing pedestrian safety and mobility in smart cities.},
  keywords  = {Sensing Framework, Multimodal Sensing, Smart City, Pedestrian Mobility},
  url       = {https://mc.manuscriptcentral.com/sensys},
  note      = {Manuscript ID: SenSys-12345-2024}
}


@misc{sun2024gexse,
  author    = {Yuan Sun and Murtadha Aldeer and Jorge Ortiz},
  title     = {GeXSe (Generative Explanatory Sensor System): An Interpretable Deep Generative Model for Human Activity Recognition in Smart Spaces},
  note      = {Under submission}, 
  year      = {2024},
  month     = {April},
  abstract  = {Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence—approaches that are often impractical in real-world settings. In response, we introduce GeXSe (Generative Explanatory Sensor System), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, GeXSe elucidates the reasoning behind activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly available datasets demonstrates that GeXSe enhances the accuracy of complex activity recognition without necessitating precise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that GeXSe’s explanations are more intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among non-experts.},
  keywords  = {Explainable IoT, Deep generative model, Hardware deep learning, Deep learning on small dataset},
  url       = {https://mc.manuscriptcentral.com/iot},
  note      = {Manuscript ID: IoT-36894-2024}
}



@misc{chowdhury2024designing,
      title={Designing a User-centric Framework for Information Quality Ranking of Large-scale Street View Images}, 
      author={Tahiya Chowdhury and Ilan Mandel and Jorge Ortiz and Wendy Ju},
      year={2024},
      eprint={2404.00392},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@unpublished{Sun2024,
  author    = {Yuan Sun and Navid Salami Pargoo and Taqiya Ehsan and Zhao Zhang and Jorge Ortiz},
  title     = {VCHAR: Variance-Driven Complex Human Activity Recognition Framework with Generative Representation},
  note      = {Under submission},
  year      = {2024},
  month     = {May},
  abstract  = {Complex human activity recognition (CHAR) remains a pivotal challenge within ubiquitous computing, especially in the context of smart environments. Existing studies typically require meticulous labeling of both atomic and complex activities, a task that is labor-intensive and prone to errors due to the scarcity and inaccuracies of available datasets. Most prior research has focused on datasets that either precisely label atomic activities or, at minimum, their sequence—approaches that are often impractical in real-world settings. In response, we introduce VCHAR (Variance-Driven Complex Human Activity Recognition), a novel framework that treats the outputs of atomic activities as a distribution over specified intervals. Leveraging generative methodologies, VCHAR elucidates the reasoning behind activity classifications through video-based explanations, accessible to users without prior machine learning expertise. Our evaluation across three publicly available datasets demonstrates that VCHAR enhances the accuracy of complex activity recognition without necessitating precise temporal or sequential labeling of atomic activities. Furthermore, user studies confirm that VCHAR’s explanations are more intelligible compared to existing methods, facilitating a broader understanding of complex activity recognition among non-experts.},
  keywords  = {Explainable IoT, Deep generative model, Hardware deep learning, Deep learning on small dataset}
}

